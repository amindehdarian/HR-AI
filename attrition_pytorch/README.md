# Attrition Prediction (PyTorch)
### A Step-by-Step Educational Example for Workforce Analytics

This project implements a simple **binary classification model** in **PyTorch** to predict employee attrition (leaving vs. staying).  
Although the notebook uses **synthetic data**, the full workflow is designed to mirror a **real-world HR analytics pipeline**.

The README is written as a **step-by-step guide for graduate students**, explaining:

- the logic behind the code  
- how the model is structured and trained  
- how to interpret the outputs in an HR context  

---

## 1. Conceptual Background

### 1.1. What is Attrition Prediction?

Employee attrition prediction aims to estimate the probability that an employee will **voluntarily leave** an organization within a given period. This is typically framed as a **binary classification problem**:

- `0` = employee stays  
- `1` = employee leaves  

Such models can support:

- workforce planning  
- targeted retention measures  
- understanding risk patterns across units, roles, or demographics  

### 1.2. Why a Neural Network?

Many models can be used for attrition (logistic regression, tree-based models, etc.).  
Here we choose a **small feed-forward neural network** to:

- illustrate the PyTorch workflow (tensors, layers, forward pass, training loop)  
- show how to move from traditional statistics to more flexible function approximators  

The goal is **educational**, not to claim that this is the “best” model for attrition.

---

## 2. Data and Feature Construction

This section describes how the dataset used for the attrition model is structured and how input features and labels are generated and represented within the modeling pipeline. The goal is to create a clear understanding of how employee-level information is transformed into tensors that can be used by a predictive model.

---

### 2.1 Feature Representation

We assume a dataset with `n_samples` employees and `n_features` numerical variables that commonly appear in HR analytics, such as:

- age  
- tenure (years)  
- engagement or sentiment measures  
- manager rating  
- compensation indicators  

These features are arranged into a matrix `X` with shape:

\[
X \in \mathbb{R}^{n\_samples \times n\_features}
\]

In code:

```python
X = torch.randn(n_samples, n_features)

Each row corresponds to one employee, and each column corresponds to one HR-related predictor.

---

### 2.2 Latent Relationship Between Features and Attrition

To illustrate how predictive modeling identifies patterns, the example constructs an underlying relationship between employee characteristics and the likelihood of leaving the organization. This enables the model to learn systematic associations between predictors (e.g., engagement, tenure) and attrition outcomes.

The assumptions encoded in this relationship follow common HR analytics reasoning, such as:

- higher engagement → lower likelihood of leaving  
- higher manager rating → lower likelihood of leaving  
- very short tenure → potentially elevated turnover risk  

These assumptions are implemented through a set of underlying weights:

```python
weights_true = torch.tensor([...])
logits = X @ weights_true + noise
probs = torch.sigmoid(logits)

The transformation from logits to probs ensures that all predicted values fall within the interval [0,1], which can be interpreted as the likelihood of an employee leaving.

Attrition labels are then generated by sampling from these probabilities:

y = torch.bernoulli(probs).long()

This produces a binary target vector:

0 → employee stays

1 → employee leaves

The tensors X and y form the input–output pair required for training a predictive model.

### 2.3 Rationale for This Construction

This feature–outcome configuration provides a clear and controlled foundation for understanding how predictive models operate on HR-related data. By defining an underlying relationship between employee characteristics and attrition likelihood, the model can identify patterns that resemble those commonly explored in workforce analytics.

This approach allows us to:

- establish a transparent mapping between predictors and attrition probability  
- demonstrate how neural networks capture relationships in employee data  
- observe how variations in feature values influence predicted outcomes  
- maintain a structure consistent with the types of variables used in HRIS datasets for retention and turnover analysis  

The resulting tensors `X` (features) and `y` (attrition labels) form the essential input–output pair required for model training. In real-world applications, these tensors can be replaced with actual HR data following standard preprocessing steps such as scaling numerical variables, encoding categorical attributes, and addressing missing values.

